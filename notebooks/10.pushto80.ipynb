{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c1ad64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded. Ready to ensemble.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# IMPORTANT: Update these paths to the final checkpoints of your two best runs\n",
    "MODEL_1_PATH = './results/final_tune_random_deletion/checkpoint-505' #<-- CHECK AND UPDATE THIS!\n",
    "MODEL_2_PATH = './results/final_run_clean_data_backtranslation/checkpoint-404' #<-- CHECK AND UPDATE THIS!\n",
    "\n",
    "FRIENDS_DATA_PATH = '../data/data1.xlsx'\n",
    "\n",
    "print(\"Configuration loaded. Ready to ensemble.\")\n",
    "# Tip: Find the checkpoint number by looking inside the results folders for each run.\n",
    "# It's usually the folder with the highest number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1450cde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models and tokenizer...\n",
      "Loaded Model 1 from: ./results/final_tune_random_deletion/checkpoint-505\n",
      "Loaded Model 2 from: ./results/final_run_clean_data_backtranslation/checkpoint-404\n",
      "\n",
      "Loading and preparing the sacred test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 9092.26 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 test entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Both Models and the Test Data\n",
    "\n",
    "print(\"Loading models and tokenizer...\")\n",
    "# We only need one tokenizer as both models share the same architecture\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_2_PATH)\n",
    "\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained(MODEL_1_PATH)\n",
    "trainer1 = Trainer(model=model1)\n",
    "print(f\"Loaded Model 1 from: {MODEL_1_PATH}\")\n",
    "\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(MODEL_2_PATH)\n",
    "trainer2 = Trainer(model=model2)\n",
    "print(f\"Loaded Model 2 from: {MODEL_2_PATH}\")\n",
    "\n",
    "print(\"\\nLoading and preparing the sacred test set...\")\n",
    "# Load the CLEANED friends dataset\n",
    "df_friends = pd.read_excel(FRIENDS_DATA_PATH)\n",
    "\n",
    "# --- Data Cleaning (must be identical to your previous runs) ---\n",
    "df_friends.columns = [col.strip().lower() for col in df_friends.columns]\n",
    "if 'entry' in df_friends.columns: df_friends.rename(columns={'entry': 'text'}, inplace=True)\n",
    "df_friends.dropna(subset=['text', 'emotion'], inplace=True)\n",
    "df_friends.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "# --- Recreate the EXACT same test set ---\n",
    "test_df = df_friends.sample(frac=0.2, random_state=42)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# --- Preprocessing (must be identical to your last run - NO demoji) ---\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_test_ds = test_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "\n",
    "print(f\"Loaded {len(test_df)} test entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b16d2f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions from Model 1 (Random Deletion)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions from Model 2 (Back-Translation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 31.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual predictions complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Get Predictions from Each Model\n",
    "\n",
    "print(\"Getting predictions from Model 1 (Random Deletion)...\")\n",
    "predictions1 = trainer1.predict(tokenized_test_ds)\n",
    "logits1 = torch.from_numpy(predictions1.predictions)\n",
    "\n",
    "print(\"Getting predictions from Model 2 (Back-Translation)...\")\n",
    "predictions2 = trainer2.predict(tokenized_test_ds)\n",
    "logits2 = torch.from_numpy(predictions2.predictions)\n",
    "\n",
    "print(\"Individual predictions complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc5b3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ensembling Results ---\n",
      "\n",
      "--- Final Ensemble Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger     0.7143    0.8000    0.7547        25\n",
      "     Disgust     0.8462    0.6875    0.7586        16\n",
      "        Fear     0.8571    0.8000    0.8276        30\n",
      "         Joy     0.9020    0.9020    0.9020        51\n",
      "     Neutral     0.7188    0.6765    0.6970        34\n",
      "     Sadness     0.8125    0.8864    0.8478        44\n",
      "\n",
      "    accuracy                         0.8150       200\n",
      "   macro avg     0.8085    0.7920    0.7979       200\n",
      "weighted avg     0.8165    0.8150    0.8142       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Previous Best Single Model F1 Score: 0.7570\n",
      "Final Ensemble F1 Score: 0.8142\n",
      "\n",
      "SUCCESS! The ensemble strategy improved performance!\n",
      "MILESTONE ACHIEVED: You have broken the 80% F1-score barrier!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Ensemble Predictions and Evaluate Final Performance\n",
    "\n",
    "print(\"\\n--- Ensembling Results ---\")\n",
    "\n",
    "# The Ensemble Strategy: Average the logits from both models\n",
    "ensembled_logits = (logits1 + logits2) / 2.0\n",
    "\n",
    "# Get the final predicted class indices from the ensembled logits\n",
    "ensembled_preds_indices = torch.argmax(ensembled_logits, axis=1).numpy()\n",
    "\n",
    "# Get the string labels for the predictions\n",
    "# Use the config from our best model (model2) to map indices to labels\n",
    "ensembled_preds_labels = [model2.config.id2label[i] for i in ensembled_preds_indices]\n",
    "\n",
    "# Get the true labels\n",
    "y_true = test_df['emotion'].tolist()\n",
    "\n",
    "# Generate and print the final classification report\n",
    "print(\"\\n--- Final Ensemble Classification Report ---\")\n",
    "print(classification_report(y_true, ensembled_preds_labels, digits=4))\n",
    "\n",
    "# Extract the final weighted F1-score to compare\n",
    "report = classification_report(y_true, ensembled_preds_labels, output_dict=True)\n",
    "ensemble_f1 = report['weighted avg']['f1-score']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Previous Best Single Model F1 Score: 0.7570\")\n",
    "print(f\"Final Ensemble F1 Score: {ensemble_f1:.4f}\")\n",
    "\n",
    "if ensemble_f1 > 0.7570:\n",
    "    print(\"\\nSUCCESS! The ensemble strategy improved performance!\")\n",
    "    if ensemble_f1 >= 0.80:\n",
    "        print(\"MILESTONE ACHIEVED: You have broken the 80% F1-score barrier!\")\n",
    "else:\n",
    "    print(\"\\nThe ensemble performance was similar to the best single model.\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-lab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
