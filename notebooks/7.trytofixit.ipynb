{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "686282e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete. Setup ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "import demoji\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import random # Add this import for the new augmentation\n",
    "\n",
    "print(\"Imports complete. Setup ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c40c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced setup with Random Deletion complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: New Configuration and Augmentation Setup\n",
    "\n",
    "# --- NEW CONFIGURATION ---\n",
    "MODEL_CHECKPOINT = \"cardiffnlp/twitter-roberta-base-emotion-multilabel-latest\"\n",
    "RUN_NAME = \"final_tune_random_deletion\" # A new name for our final run\n",
    "\n",
    "# --- Define paths to your data ---\n",
    "FRIENDS_DATA_PATH = '../data/data1.xlsx'\n",
    "KAGGLE_DATA_PATHS = ['../data/data2.xlsx', '../data/data3.xlsx']\n",
    "\n",
    "# --- NEW: Setup Random Deletion Augmenter ---\n",
    "# This function requires no new libraries!\n",
    "def augment_with_random_deletion(df, p=0.15, num_augmented_samples=1000):\n",
    "    augmented_texts = []\n",
    "    original_df = df.copy()\n",
    "    \n",
    "    print(f\"Generating {num_augmented_samples} new samples via Random Deletion...\")\n",
    "    # Make sure there is text to sample from\n",
    "    if len(original_df.dropna(subset=['text'])) == 0:\n",
    "        print(\"Warning: DataFrame to augment is empty or contains no text. Skipping augmentation.\")\n",
    "        return original_df\n",
    "\n",
    "    for i in range(num_augmented_samples):\n",
    "        # Sample only from rows that have valid text\n",
    "        sample = original_df.dropna(subset=['text']).sample(1)\n",
    "        original_text = sample['text'].iloc[0]\n",
    "        original_emotion = sample['emotion'].iloc[0]\n",
    "        \n",
    "        words = str(original_text).split()\n",
    "        # Only augment if the text is reasonably long\n",
    "        if len(words) < 5: \n",
    "            continue\n",
    "            \n",
    "        # Keep each word with probability (1-p)\n",
    "        new_words = [word for word in words if random.random() > p]\n",
    "        \n",
    "        if len(new_words) > 0:\n",
    "            new_text = \" \".join(new_words)\n",
    "            augmented_texts.append({'text': new_text, 'emotion': original_emotion})\n",
    "            \n",
    "    return pd.concat([original_df, pd.DataFrame(augmented_texts)], ignore_index=True)\n",
    "\n",
    "print(\"Advanced setup with Random Deletion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47afa88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 1: Preparing Datasets with Augmentation ---\n",
      "Generating 1000 new samples via Random Deletion...\n",
      "\n",
      "Augmented General Training Pool size: 3486\n",
      "Specialist Training set (friends only): 801\n",
      "Validation set size: 279\n",
      "Sacred Test set (friends only): 200\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Full Data Preparation with New Augmentation\n",
    "\n",
    "print(\"\\n--- STEP 1: Preparing Datasets with Augmentation ---\")\n",
    "\n",
    "# Load and combine data\n",
    "df_friends = pd.read_excel(FRIENDS_DATA_PATH)\n",
    "df_kaggle_list = [pd.read_excel(p) for p in KAGGLE_DATA_PATHS]\n",
    "df_kaggle = pd.concat(df_kaggle_list, ignore_index=True)\n",
    "\n",
    "# Standardize and rename columns\n",
    "df_friends.columns = [col.strip().lower() for col in df_friends.columns]\n",
    "df_kaggle.columns = [col.strip().lower() for col in df_kaggle.columns]\n",
    "if 'entry' in df_friends.columns: df_friends.rename(columns={'entry': 'text'}, inplace=True)\n",
    "if 'entry' in df_kaggle.columns: df_kaggle.rename(columns={'entry': 'text'}, inplace=True)\n",
    "\n",
    "# Clean and drop duplicates\n",
    "df_friends.dropna(subset=['text', 'emotion'], inplace=True)\n",
    "df_kaggle.dropna(subset=['text', 'emotion'], inplace=True)\n",
    "df_friends.drop_duplicates(subset=['text'], inplace=True)\n",
    "df_kaggle.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "# Create the same strategic split as before\n",
    "test_df = df_friends.sample(frac=0.2, random_state=42)\n",
    "train_friends_df = df_friends.drop(test_df.index) # Specialist data\n",
    "train_pool_df = pd.concat([train_friends_df, df_kaggle], ignore_index=True)\n",
    "train_pool_df, val_df = train_test_split(train_pool_df, test_size=0.1, random_state=42, stratify=train_pool_df['emotion'])\n",
    "\n",
    "# --- APPLY THE NEW AUGMENTATION ---\n",
    "train_pool_df = augment_with_random_deletion(train_pool_df)\n",
    "# ------------------------------------\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_pool_ds = Dataset.from_pandas(train_pool_df)\n",
    "train_friends_ds = Dataset.from_pandas(train_friends_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(f\"\\nAugmented General Training Pool size: {len(train_pool_df)}\")\n",
    "print(f\"Specialist Training set (friends only): {len(train_friends_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Sacred Test set (friends only): {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394fe442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\das99\\AppData\\Local\\Temp\\ipykernel_7324\\1337574643.py:6: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 2: Final Preprocessing and Setup ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3486/3486 [00:01<00:00, 2821.93 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 801/801 [00:00<00:00, 1798.69 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 279/279 [00:00<00:00, 2568.66 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 1496.99 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3486/3486 [00:00<00:00, 20063.59 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 801/801 [00:00<00:00, 21641.57 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 279/279 [00:00<00:00, 14680.67 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 16521.14 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Ready for Two-Stage Training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Preprocessing, Custom Trainer, and Tokenization\n",
    "\n",
    "print(\"\\n--- STEP 2: Final Preprocessing and Setup ---\")\n",
    "\n",
    "# Initialize demoji for emoji translation\n",
    "demoji.download_codes()\n",
    "\n",
    "# Create label mappings from the full training data\n",
    "unique_labels = sorted(train_pool_df['emotion'].unique())\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "def preprocess_text_and_labels(batch):\n",
    "    # Convert emojis to text descriptions\n",
    "    batch['text'] = [demoji.replace_with_desc(str(text), sep=\" \") for text in batch['text']]\n",
    "    # Encode labels\n",
    "    batch['label'] = [label2id[label] for label in batch['emotion']]\n",
    "    return batch\n",
    "\n",
    "# Apply preprocessing to all datasets\n",
    "train_pool_ds = train_pool_ds.map(preprocess_text_and_labels, batched=True, remove_columns=['emotion'])\n",
    "train_friends_ds = train_friends_ds.map(preprocess_text_and_labels, batched=True, remove_columns=['emotion'])\n",
    "val_ds = val_ds.map(preprocess_text_and_labels, batched=True, remove_columns=['emotion'])\n",
    "test_ds = test_ds.map(preprocess_text_and_labels, batched=True, remove_columns=['emotion'])\n",
    "\n",
    "# Calculate class weights for handling imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(unique_labels),\n",
    "    y=train_pool_df['emotion'] # Use the augmented pool for weights\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\")\n",
    "\n",
    "# Create a Custom Trainer to use the class weights\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Define a shared metric computation function\n",
    "metric = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "\n",
    "# Initialize tokenizer and tokenize datasets\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_train_pool_ds = train_pool_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "tokenized_train_friends_ds = train_friends_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "tokenized_val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "tokenized_test_ds = test_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "\n",
    "print(\"Setup complete. Ready for Two-Stage Training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae47fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STARTING TWO-STAGE TRAINING: final_tune_random_deletion\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion-multilabel-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 1: Training on General Pool (Friends + Kaggle + Augmented) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|â–ˆâ–ˆ        | 218/1090 [00:49<03:11,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9040754437446594, 'eval_f1': 0.6777127477305828, 'eval_runtime': 1.252, 'eval_samples_per_second': 222.844, 'eval_steps_per_second': 27.955, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 436/1090 [01:41<02:22,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8899373412132263, 'eval_f1': 0.7038165537003402, 'eval_runtime': 1.2994, 'eval_samples_per_second': 214.71, 'eval_steps_per_second': 26.935, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 500/1090 [01:57<02:13,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9479, 'grad_norm': 25.551973342895508, 'learning_rate': 1.6238532110091743e-05, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 654/1090 [02:34<01:39,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9623381495475769, 'eval_f1': 0.7327338213899331, 'eval_runtime': 1.3662, 'eval_samples_per_second': 204.214, 'eval_steps_per_second': 25.618, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 872/1090 [03:28<00:52,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9936513304710388, 'eval_f1': 0.7297673536646305, 'eval_runtime': 1.4367, 'eval_samples_per_second': 194.189, 'eval_steps_per_second': 24.361, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1001/1090 [04:01<00:21,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.374, 'grad_norm': 41.7637825012207, 'learning_rate': 2.4770642201834866e-06, 'epoch': 4.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1090/1090 [04:24<00:00,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0543166399002075, 'eval_f1': 0.7412929737899686, 'eval_runtime': 1.409, 'eval_samples_per_second': 198.008, 'eval_steps_per_second': 24.84, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1090/1090 [04:26<00:00,  4.09it/s]\n",
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 266.0447, 'train_samples_per_second': 65.515, 'train_steps_per_second': 4.097, 'train_loss': 0.626404221560977, 'epoch': 5.0}\n",
      "\n",
      "--- STAGE 2: Fine-tuning on Specialist Data (Friends Only) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–Ž        | 101/808 [00:14<01:35,  7.39it/s]\n",
      " 12%|â–ˆâ–Ž        | 101/808 [00:16<01:35,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0658799409866333, 'eval_f1': 0.8004439240028748, 'eval_runtime': 1.3888, 'eval_samples_per_second': 200.899, 'eval_steps_per_second': 25.202, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–       | 201/808 [00:32<01:28,  6.85it/s]\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 202/808 [00:33<01:28,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1217749118804932, 'eval_f1': 0.7913037909410715, 'eval_runtime': 1.38, 'eval_samples_per_second': 202.167, 'eval_steps_per_second': 25.361, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 302/808 [00:49<01:15,  6.68it/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 303/808 [00:51<01:15,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1040960550308228, 'eval_f1': 0.7965370697816657, 'eval_runtime': 1.3935, 'eval_samples_per_second': 200.21, 'eval_steps_per_second': 25.116, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 403/808 [01:08<00:59,  6.84it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 404/808 [01:09<00:59,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1084553003311157, 'eval_f1': 0.8123984948187972, 'eval_runtime': 1.3842, 'eval_samples_per_second': 201.563, 'eval_steps_per_second': 25.286, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 501/808 [01:25<00:44,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.199, 'grad_norm': 0.14616155624389648, 'learning_rate': 3.8118811881188123e-06, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 504/808 [01:25<00:44,  6.89it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 505/808 [01:27<00:44,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2167236804962158, 'eval_f1': 0.8165956546253277, 'eval_runtime': 1.401, 'eval_samples_per_second': 199.141, 'eval_steps_per_second': 24.982, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 605/808 [01:43<00:30,  6.61it/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 606/808 [01:45<00:30,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2397997379302979, 'eval_f1': 0.8103734038569294, 'eval_runtime': 1.4201, 'eval_samples_per_second': 196.465, 'eval_steps_per_second': 24.646, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 707/808 [02:01<00:13,  7.43it/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 707/808 [02:02<00:13,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2387895584106445, 'eval_f1': 0.8100678991840738, 'eval_runtime': 1.4187, 'eval_samples_per_second': 196.66, 'eval_steps_per_second': 24.671, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 807/808 [02:19<00:00,  6.74it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 808/808 [02:20<00:00,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2494066953659058, 'eval_f1': 0.8104053634802086, 'eval_runtime': 1.425, 'eval_samples_per_second': 195.792, 'eval_steps_per_second': 24.562, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 808/808 [02:23<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 142.9412, 'train_samples_per_second': 44.83, 'train_steps_per_second': 5.653, 'train_loss': 0.1428397327366442, 'epoch': 8.0}\n",
      "\n",
      "--- Evaluating the final SPECIALIST model on the sacred test set ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 25.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "--- FINAL EXPERIMENT COMPLETE ---\n",
      "\n",
      "Final Model: final_tune_random_deletion\n",
      "Previous Best F1 Score: 0.69\n",
      "Final Specialist F1 Score on Friends Data: 0.7250\n",
      "\n",
      "Your final, best specialist model is saved in: ./results/final_tune_random_deletion\n",
      "\n",
      "SUCCESS! The new strategy improved performance.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Two-Stage Training and Final Evaluation\n",
    "\n",
    "print(f\"\\n{'='*50}\\nSTARTING TWO-STAGE TRAINING: {RUN_NAME}\\n{'='*50}\\n\")\n",
    "\n",
    "# Load the initial model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True # Important for multilabel -> single-label\n",
    ")\n",
    "\n",
    "output_dir = f\"./results/{RUN_NAME}\"\n",
    "\n",
    "# --- STAGE 1: Generalist Training on Augmented Data ---\n",
    "print(\"\\n--- STAGE 1: Training on General Pool (Friends + Kaggle + Augmented) ---\")\n",
    "stage1_args = TrainingArguments(\n",
    "    output_dir=f\"{output_dir}/stage1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5, # 5 epochs should be enough for the larger dataset\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer_stage1 = CustomTrainer(\n",
    "    model=model,\n",
    "    args=stage1_args,\n",
    "    train_dataset=tokenized_train_pool_ds,\n",
    "    eval_dataset=tokenized_val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "trainer_stage1.train()\n",
    "\n",
    "\n",
    "# --- STAGE 2: Specialist Training on Friends Data ---\n",
    "print(\"\\n--- STAGE 2: Fine-tuning on Specialist Data (Friends Only) ---\")\n",
    "stage2_args = TrainingArguments(\n",
    "    output_dir=output_dir, # Save the FINAL model here\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5, # Use a smaller learning rate for specialization\n",
    "    per_device_train_batch_size=8, # A smaller batch size can help on small datasets\n",
    "    num_train_epochs=8, # More epochs on the specialist data\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer_stage2 = CustomTrainer(\n",
    "    model=trainer_stage1.model, # Use the model that finished Stage 1\n",
    "    args=stage2_args,\n",
    "    train_dataset=tokenized_train_friends_ds,\n",
    "    eval_dataset=tokenized_val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)], # More patience here\n",
    ")\n",
    "\n",
    "trainer_stage2.train()\n",
    "\n",
    "# --- FINAL EVALUATION ---\n",
    "print(\"\\n--- Evaluating the final SPECIALIST model on the sacred test set ---\")\n",
    "test_results = trainer_stage2.evaluate(eval_dataset=tokenized_test_ds)\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\\n--- FINAL EXPERIMENT COMPLETE ---\\n\")\n",
    "print(f\"Final Model: {RUN_NAME}\")\n",
    "print(f\"Previous Best F1 Score: 0.69\") # Our score to beat\n",
    "print(f\"Final Specialist F1 Score on Friends Data: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"\\nYour final, best specialist model is saved in: {output_dir}\")\n",
    "\n",
    "if test_results['eval_f1'] > 0.69:\n",
    "    print(\"\\nSUCCESS! The new strategy improved performance.\")\n",
    "else:\n",
    "    print(\"\\nPerformance was similar to the previous run. This is still a highly robust model.\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-lab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
