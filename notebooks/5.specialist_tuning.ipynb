{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8686b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete. Setup ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup (No Changes)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "import demoji\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    AdamW\n",
    ")\n",
    "\n",
    "print(\"Imports complete. Setup ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b6bb6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration (No Changes)\n",
    "MODEL_CHECKPOINT = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "FRIENDS_DATA_PATH = '../data/data1.xlsx'\n",
    "KAGGLE_DATA_PATHS = ['../data/data2.xlsx', '../data/data3.xlsx']\n",
    "RUN_NAME = \"specialist_two_stage_tune\"\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3002f66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STEP 1: Preparing Datasets for Two-Stage Training ---\n",
      "General Training Pool size: 2509\n",
      "Specialist Training set (friends only): 801\n",
      "Validation set size: 279\n",
      "Sacred Test set (friends only): 200\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Loading and Splitting (Slight Modification)\n",
    "print(\"--- STEP 1: Preparing Datasets for Two-Stage Training ---\")\n",
    "# Load and clean data\n",
    "df_friends = pd.read_excel(FRIENDS_DATA_PATH)\n",
    "df_kaggle_list = [pd.read_excel(p) for p in KAGGLE_DATA_PATHS]\n",
    "df_kaggle = pd.concat(df_kaggle_list, ignore_index=True)\n",
    "df_friends.columns = [col.strip().lower() for col in df_friends.columns]\n",
    "df_kaggle.columns = [col.strip().lower() for col in df_kaggle.columns]\n",
    "if 'entry' in df_friends.columns:\n",
    "    df_friends.rename(columns={'entry': 'text'}, inplace=True)\n",
    "if 'entry' in df_kaggle.columns:\n",
    "    df_kaggle.rename(columns={'entry': 'text'}, inplace=True)\n",
    "df_friends.dropna(subset=['text', 'emotion'], inplace=True)\n",
    "df_kaggle.dropna(subset=['text', 'emotion'], inplace=True)\n",
    "df_friends.drop_duplicates(subset=['text'], inplace=True)\n",
    "df_kaggle.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "# Strategic Split\n",
    "test_df = df_friends.sample(frac=0.2, random_state=42)\n",
    "train_friends_df = df_friends.drop(test_df.index)\n",
    "train_pool_df = pd.concat([train_friends_df, df_kaggle], ignore_index=True)\n",
    "train_pool_df, val_df = train_test_split(train_pool_df, test_size=0.1, random_state=42, stratify=train_pool_df['emotion'])\n",
    "\n",
    "# We now have THREE dataframes for training:\n",
    "# 1. train_pool_df: The large general dataset (Friends + Kaggle)\n",
    "# 2. train_friends_df: The smaller specialist dataset (Friends only)\n",
    "# 3. val_df: The validation set (from the general pool)\n",
    "# 4. test_df: The sacred test set (from friends only)\n",
    "\n",
    "print(f\"General Training Pool size: {len(train_pool_df)}\")\n",
    "print(f\"Specialist Training set (friends only): {len(train_friends_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Sacred Test set (friends only): {len(test_df)}\")\n",
    "\n",
    "# Convert to Hugging Face Datasets\n",
    "train_pool_ds = Dataset.from_pandas(train_pool_df)\n",
    "train_friends_ds = Dataset.from_pandas(train_friends_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50157840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\das99\\AppData\\Local\\Temp\\ipykernel_17648\\3486460699.py:3: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 2: Final Preprocessing and Setup ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2509/2509 [00:01<00:00, 2096.78 examples/s]\n",
      "Map: 100%|██████████| 801/801 [00:00<00:00, 1445.16 examples/s]\n",
      "Map: 100%|██████████| 279/279 [00:00<00:00, 2359.07 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 1350.31 examples/s]\n",
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 2509/2509 [00:00<00:00, 11386.91 examples/s]\n",
      "Map: 100%|██████████| 801/801 [00:00<00:00, 21606.08 examples/s]\n",
      "Map: 100%|██████████| 279/279 [00:00<00:00, 18571.53 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 16647.04 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Ready for Two-Stage Training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Preprocessing and Setup (No Changes)\n",
    "print(\"\\n--- STEP 2: Final Preprocessing and Setup ---\")\n",
    "demoji.download_codes()\n",
    "def preprocess_text_and_labels(batch):\n",
    "    batch['text'] = [demoji.replace_with_desc(str(text), sep=\" \") for text in batch['text']]\n",
    "    batch['label'] = [label2id[label] for label in batch['emotion']]\n",
    "    return batch\n",
    "\n",
    "unique_labels = train_pool_df['emotion'].unique()\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "# Apply preprocessing to all datasets\n",
    "train_pool_ds = train_pool_ds.map(preprocess_text_and_labels, batched=True)\n",
    "train_friends_ds = train_friends_ds.map(preprocess_text_and_labels, batched=True)\n",
    "val_ds = val_ds.map(preprocess_text_and_labels, batched=True)\n",
    "test_ds = test_ds.map(preprocess_text_and_labels, batched=True)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.array(list(label2id.keys())), y=train_pool_df['emotion'])\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "metric = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_train_pool_ds = train_pool_ds.map(tokenize_fn, batched=True)\n",
    "tokenized_train_friends_ds = train_friends_ds.map(tokenize_fn, batched=True)\n",
    "tokenized_val_ds = val_ds.map(tokenize_fn, batched=True)\n",
    "tokenized_test_ds = test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "print(\"Setup complete. Ready for Two-Stage Training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb81b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STARTING TWO-STAGE TRAINING: specialist_two_stage_tune\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([4, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 1: Training on General Pool (Friends + Kaggle) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 157/628 [00:47<02:17,  3.42it/s]\n",
      " 25%|██▌       | 157/628 [00:48<02:17,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9491251111030579, 'eval_f1': 0.699398630570837, 'eval_runtime': 1.6556, 'eval_samples_per_second': 168.515, 'eval_steps_per_second': 21.14, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 314/628 [01:55<01:57,  2.68it/s]\n",
      " 50%|█████     | 314/628 [01:57<01:57,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8604706525802612, 'eval_f1': 0.7403891023656725, 'eval_runtime': 2.6594, 'eval_samples_per_second': 104.912, 'eval_steps_per_second': 13.161, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 471/628 [03:00<01:00,  2.57it/s]\n",
      " 75%|███████▌  | 471/628 [03:02<01:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8669783473014832, 'eval_f1': 0.738660690144155, 'eval_runtime': 1.9336, 'eval_samples_per_second': 144.291, 'eval_steps_per_second': 18.101, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 500/628 [03:18<00:55,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8451, 'grad_norm': 6.169551849365234, 'learning_rate': 6.114649681528663e-06, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 628/628 [04:02<00:00,  3.07it/s]\n",
      "100%|██████████| 628/628 [04:05<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8949955701828003, 'eval_f1': 0.745667436341887, 'eval_runtime': 2.3235, 'eval_samples_per_second': 120.08, 'eval_steps_per_second': 15.064, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 628/628 [04:09<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 249.7721, 'train_samples_per_second': 40.181, 'train_steps_per_second': 2.514, 'train_loss': 0.7610877395435504, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 628/628 [04:10<00:00,  2.51it/s]\n",
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 2: Fine-tuning on Specialist Data (Friends Only) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 101/606 [00:27<02:19,  3.62it/s]\n",
      " 17%|█▋        | 101/606 [00:29<02:19,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8314382433891296, 'eval_f1': 0.7886173534495715, 'eval_runtime': 2.217, 'eval_samples_per_second': 125.843, 'eval_steps_per_second': 15.787, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 202/606 [00:56<02:18,  2.91it/s]\n",
      " 33%|███▎      | 202/606 [00:58<02:18,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8567943572998047, 'eval_f1': 0.8083294503562307, 'eval_runtime': 2.8122, 'eval_samples_per_second': 99.21, 'eval_steps_per_second': 12.446, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 303/606 [01:37<02:21,  2.15it/s]\n",
      " 50%|█████     | 303/606 [01:39<02:21,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9350295066833496, 'eval_f1': 0.8047111907412341, 'eval_runtime': 2.2499, 'eval_samples_per_second': 124.005, 'eval_steps_per_second': 15.556, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 404/606 [02:09<00:51,  3.89it/s]\n",
      " 67%|██████▋   | 404/606 [02:11<00:51,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.939440131187439, 'eval_f1': 0.8201078033639704, 'eval_runtime': 1.9943, 'eval_samples_per_second': 139.902, 'eval_steps_per_second': 17.55, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 501/606 [02:38<00:25,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2706, 'grad_norm': 0.4594554305076599, 'learning_rate': 1.7491749174917493e-06, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 505/606 [02:39<00:28,  3.59it/s]\n",
      " 83%|████████▎ | 505/606 [02:42<00:28,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9866503477096558, 'eval_f1': 0.813989877363965, 'eval_runtime': 2.3724, 'eval_samples_per_second': 117.605, 'eval_steps_per_second': 14.753, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 605/606 [03:05<00:00,  7.00it/s]\n",
      "100%|██████████| 606/606 [03:06<00:00,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9987325668334961, 'eval_f1': 0.8066750478935846, 'eval_runtime': 1.3378, 'eval_samples_per_second': 208.555, 'eval_steps_per_second': 26.163, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 606/606 [03:09<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 189.3307, 'train_samples_per_second': 25.384, 'train_steps_per_second': 3.201, 'train_loss': 0.24605793213293498, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=606, training_loss=0.24605793213293498, metrics={'train_runtime': 189.3307, 'train_samples_per_second': 25.384, 'train_steps_per_second': 3.201, 'total_flos': 316139286555648.0, 'train_loss': 0.24605793213293498, 'epoch': 6.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5: Two-Stage Training\n",
    "print(f\"\\n{'='*50}\\nSTARTING TWO-STAGE TRAINING: {RUN_NAME}\\n{'='*50}\\n\")\n",
    "\n",
    "# Load the initial model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "output_dir = f\"./results/{RUN_NAME}\"\n",
    "\n",
    "# --- STAGE 1: Generalist Training ---\n",
    "print(\"\\n--- STAGE 1: Training on General Pool (Friends + Kaggle) ---\")\n",
    "stage1_args = TrainingArguments(\n",
    "    output_dir=f\"{output_dir}/stage1_checkpoints\", # Save intermediate checkpoints here\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Initialize the first trainer\n",
    "trainer_stage1 = CustomTrainer(\n",
    "    model=model,\n",
    "    args=stage1_args,\n",
    "    train_dataset=tokenized_train_pool_ds,\n",
    "    eval_dataset=tokenized_val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "# Run the first stage of training\n",
    "trainer_stage1.train()\n",
    "\n",
    "# The best model from stage 1 is now loaded into trainer_stage1.model\n",
    "\n",
    "\n",
    "# --- STAGE 2: Specialist Training ---\n",
    "print(\"\\n--- STAGE 2: Fine-tuning on Specialist Data (Friends Only) ---\")\n",
    "\n",
    "stage2_args = TrainingArguments(\n",
    "    output_dir=output_dir, # Save the FINAL model to the main run directory\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# --- THIS IS THE FIX ---\n",
    "# Create a NEW Trainer for the second stage.\n",
    "# It uses the SAME model object, which now contains the weights from Stage 1.\n",
    "trainer_stage2 = CustomTrainer(\n",
    "    model=trainer_stage1.model, # Use the model that finished Stage 1\n",
    "    args=stage2_args,\n",
    "    train_dataset=tokenized_train_friends_ds,\n",
    "    eval_dataset=tokenized_val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "# --------------------\n",
    "\n",
    "# Run the second, specialist stage of training\n",
    "trainer_stage2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e81222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating the final SPECIALIST model on the sacred test set ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 27.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "--- FINAL EXPERIMENT COMPLETE ---\n",
      "\n",
      "Final Model: specialist_two_stage_tune (Two-Stage Trained)\n",
      "Previous Honest Baseline F1 Score: 0.5771\n",
      "Final Specialist F1 Score on Friends Data: 0.6665\n",
      "\n",
      "Your final, best specialist model is saved in: ./results/specialist_two_stage_tune\n",
      "\n",
      "SUCCESS! The two-stage training strategy improved performance.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Final Evaluation\n",
    "print(\"\\n--- Evaluating the final SPECIALIST model on the sacred test set ---\")\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_test_ds)\n",
    "print(f\"\\n\\n{'='*60}\\n--- FINAL EXPERIMENT COMPLETE ---\\n\")\n",
    "print(f\"Final Model: {RUN_NAME} (Two-Stage Trained)\")\n",
    "print(f\"Previous Honest Baseline F1 Score: 0.5771\")\n",
    "print(f\"Final Specialist F1 Score on Friends Data: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"\\nYour final, best specialist model is saved in: {output_dir}\")\n",
    "\n",
    "if test_results['eval_f1'] > 0.5771:\n",
    "    print(\"\\nSUCCESS! The two-stage training strategy improved performance.\")\n",
    "else:\n",
    "    print(\"\\nThis is still a strong result. The model is now highly specialized for your data.\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-lab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
