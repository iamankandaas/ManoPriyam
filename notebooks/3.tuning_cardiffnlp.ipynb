{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b51164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete. Setup ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "import demoji\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    AdamW,\n",
    "    get_scheduler\n",
    ")\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "print(\"Imports complete. Setup ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4824e3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration - SET YOUR FILE PATHS HERE\n",
    "# Define the champion model we are going to tune\n",
    "MODEL_CHECKPOINT = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "\n",
    "# Define raw data files (MAKE SURE THESE PATHS ARE CORRECT FOR YOUR NOTEBOOK)\n",
    "FRIENDS_DATA_PATH = '../data/data1.xlsx'\n",
    "KAGGLE_DATA_PATHS = ['../data/data2.xlsx', '../data/data3.xlsx']\n",
    "\n",
    "# Define a single, strong set of hyperparameters for this run\n",
    "# We will combine these with the differential learning rates later\n",
    "TRAINING_PARAMS = {\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"num_train_epochs\": 8,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"f1\",\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Define a name for this experimental run\n",
    "RUN_NAME = \"final_tune_roberta_augmented_diff_lr\"\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "219ed078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STEP 1: Preparing Datasets from Excel Files ---\n",
      "Original train set size: 2509\n",
      "Validation set size: 279\n",
      "Sacred Test set (friends only): 200\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Loading, Cleaning, and Splitting\n",
    "print(\"--- STEP 1: Preparing Datasets from Excel Files ---\")\n",
    "\n",
    "# Load data\n",
    "df_friends = pd.read_excel(FRIENDS_DATA_PATH)\n",
    "df_kaggle_list = [pd.read_excel(p) for p in KAGGLE_DATA_PATHS]\n",
    "df_kaggle = pd.concat(df_kaggle_list, ignore_index=True)\n",
    "\n",
    "# Standardize and Rename Columns\n",
    "df_friends.columns = [col.strip().lower() for col in df_friends.columns]\n",
    "df_kaggle.columns = [col.strip().lower() for col in df_kaggle.columns]\n",
    "if 'entry' in df_friends.columns:\n",
    "    df_friends.rename(columns={'entry': 'text'}, inplace=True)\n",
    "if 'entry' in df_kaggle.columns:\n",
    "    df_kaggle.rename(columns={'entry': 'text'}, inplace=True)\n",
    "\n",
    "# Clean and drop duplicates\n",
    "df_friends.dropna(subset=['text', 'emotion'], inplace=True)\n",
    "df_kaggle.dropna(subset=['text', 'emotion'], inplace=True)\n",
    "df_friends.drop_duplicates(subset=['text'], inplace=True)\n",
    "df_kaggle.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "# Strategic Split: Isolate a \"golden\" test set from friends' data\n",
    "test_df = df_friends.sample(frac=0.2, random_state=42)\n",
    "train_pool_df = pd.concat([df_friends.drop(test_df.index), df_kaggle], ignore_index=True)\n",
    "train_df, val_df = train_test_split(train_pool_df, test_size=0.1, random_state=42, stratify=train_pool_df['emotion'])\n",
    "\n",
    "print(f\"Original train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Sacred Test set (friends only): {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5380f51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 2: Augmenting Training Data ---\n",
      "Generating 1500 new samples... (This may take a while)\n",
      "  ...augmented 100/1500\n",
      "  ...augmented 200/1500\n",
      "  ...augmented 300/1500\n",
      "  ...augmented 400/1500\n",
      "  ...augmented 500/1500\n",
      "  ...augmented 600/1500\n",
      "  ...augmented 700/1500\n",
      "  ...augmented 800/1500\n",
      "  ...augmented 900/1500\n",
      "  ...augmented 1000/1500\n",
      "  ...augmented 1100/1500\n",
      "  ...augmented 1200/1500\n",
      "  ...augmented 1300/1500\n",
      "  ...augmented 1400/1500\n",
      "  ...augmented 1500/1500\n",
      "Augmented train set size: 4009\n",
      "Data augmentation and conversion to Datasets complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Data Augmentation\n",
    "print(\"\\n--- STEP 2: Augmenting Training Data ---\")\n",
    "\n",
    "# Setup Augmenters\n",
    "char_inserter = nac.KeyboardAug(aug_char_p=0.03, aug_word_p=0.01)\n",
    "word_substituter = naw.ContextualWordEmbsAug(model_path='roberta-base', action=\"substitute\", aug_p=0.15, device=\"cuda\")\n",
    "\n",
    "def augment_text(df, num_augmented_samples=1500):\n",
    "    \"\"\"Augments a dataframe to create new training samples.\"\"\"\n",
    "    augmented_texts = []\n",
    "    original_df = df.copy()\n",
    "    \n",
    "    # Generate augmented samples\n",
    "    print(f\"Generating {num_augmented_samples} new samples... (This may take a while)\")\n",
    "    for i in range(num_augmented_samples):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  ...augmented {i+1}/{num_augmented_samples}\")\n",
    "            \n",
    "        sample = original_df.sample(1)\n",
    "        original_text = sample['text'].iloc[0]\n",
    "        original_emotion = sample['emotion'].iloc[0]\n",
    "        \n",
    "        # --- THIS IS THE FIX ---\n",
    "        # Apply augmentations and take the first element [0] from the returned list\n",
    "        augmented_text_1 = char_inserter.augment(original_text)[0]\n",
    "        augmented_text_2 = word_substituter.augment(augmented_text_1)[0]\n",
    "        # -----------------------\n",
    "        \n",
    "        augmented_texts.append({'text': augmented_text_2, 'emotion': original_emotion})\n",
    "        \n",
    "    return pd.concat([original_df, pd.DataFrame(augmented_texts)], ignore_index=True)\n",
    "\n",
    "# Augment the training data\n",
    "train_df_augmented = augment_text(train_df)\n",
    "print(f\"Augmented train set size: {len(train_df_augmented)}\")\n",
    "\n",
    "# Convert all DataFrames to Hugging Face Datasets\n",
    "# This will now succeed because the 'text' column contains only strings\n",
    "train_ds = Dataset.from_pandas(train_df_augmented)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(\"Data augmentation and conversion to Datasets complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7fd9a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\das99\\AppData\\Local\\Temp\\ipykernel_30736\\2453029879.py:4: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 3: Final Preprocessing and Setup ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4009/4009 [00:02<00:00, 1892.36 examples/s]\n",
      "Map: 100%|██████████| 279/279 [00:00<00:00, 2244.92 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 1502.52 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Ready for training.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Final Preprocessing, Class Weights, and Custom Trainer\n",
    "print(\"\\n--- STEP 3: Final Preprocessing and Setup ---\")\n",
    "\n",
    "demoji.download_codes()\n",
    "def preprocess_text_and_labels(batch):\n",
    "    # Convert emojis to text descriptions\n",
    "    batch['text'] = [demoji.replace_with_desc(str(text), sep=\" \") for text in batch['text']]\n",
    "    # Encode labels\n",
    "    batch['label'] = [label2id[label] for label in batch['emotion']]\n",
    "    return batch\n",
    "\n",
    "unique_labels = train_df['emotion'].unique()\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "# Apply final preprocessing\n",
    "train_ds = train_ds.map(preprocess_text_and_labels, batched=True)\n",
    "val_ds = val_ds.map(preprocess_text_and_labels, batched=True)\n",
    "test_ds = test_ds.map(preprocess_text_and_labels, batched=True)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.array(list(label2id.keys())), y=train_df['emotion'])\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\")\n",
    "\n",
    "# Create Custom Trainer for class weights\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "metric = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "\n",
    "print(\"Setup complete. Ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b1010b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STARTING FINAL RUN: final_tune_roberta_augmented_diff_lr\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 4009/4009 [00:00<00:00, 24087.93 examples/s]\n",
      "Map: 100%|██████████| 279/279 [00:00<00:00, 19223.49 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 12391.40 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([4, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                    \n",
      " 12%|█▎        | 251/2008 [18:31<1:51:55,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.225176215171814, 'eval_f1': 0.5828963668291316, 'eval_runtime': 2.3209, 'eval_samples_per_second': 120.212, 'eval_steps_per_second': 15.08, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 500/2008 [37:44<1:55:24,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3323, 'grad_norm': 12.245803833007812, 'learning_rate': 1.50199203187251e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 25%|██▌       | 502/2008 [37:54<1:39:50,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0887625217437744, 'eval_f1': 0.5977087344465216, 'eval_runtime': 2.2654, 'eval_samples_per_second': 123.158, 'eval_steps_per_second': 15.45, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 38%|███▊      | 753/2008 [58:00<1:28:03,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0091911554336548, 'eval_f1': 0.6515686383002268, 'eval_runtime': 1.8613, 'eval_samples_per_second': 149.899, 'eval_steps_per_second': 18.804, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 1000/2008 [1:15:57<1:12:20,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0774, 'grad_norm': 15.448877334594727, 'learning_rate': 1.0039840637450198e-06, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 50%|█████     | 1004/2008 [1:16:14<1:01:41,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9652299880981445, 'eval_f1': 0.6592583349793465, 'eval_runtime': 2.1331, 'eval_samples_per_second': 130.794, 'eval_steps_per_second': 16.408, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 62%|██████▎   | 1255/2008 [1:29:19<33:39,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9431164264678955, 'eval_f1': 0.6931609790547192, 'eval_runtime': 1.5992, 'eval_samples_per_second': 174.459, 'eval_steps_per_second': 21.886, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 1500/2008 [1:42:29<26:55,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9784, 'grad_norm': 14.455154418945312, 'learning_rate': 5.059760956175299e-07, 'epoch': 5.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 75%|███████▌  | 1506/2008 [1:42:48<24:01,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9236071705818176, 'eval_f1': 0.6845298535363846, 'eval_runtime': 1.5977, 'eval_samples_per_second': 174.623, 'eval_steps_per_second': 21.906, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 88%|████████▊ | 1757/2008 [1:53:44<10:11,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9201882481575012, 'eval_f1': 0.6878212087259226, 'eval_runtime': 1.6321, 'eval_samples_per_second': 170.947, 'eval_steps_per_second': 21.445, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2000/2008 [2:03:54<00:20,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9282, 'grad_norm': 12.525500297546387, 'learning_rate': 7.96812749003984e-09, 'epoch': 7.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|██████████| 2008/2008 [2:04:15<00:00,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.915812075138092, 'eval_f1': 0.6885497897890427, 'eval_runtime': 1.6279, 'eval_samples_per_second': 171.389, 'eval_steps_per_second': 21.5, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2008/2008 [2:04:18<00:00,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 7457.9486, 'train_samples_per_second': 4.3, 'train_steps_per_second': 0.269, 'train_loss': 1.078308966292803, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2008, training_loss=1.078308966292803, metrics={'train_runtime': 7457.9486, 'train_samples_per_second': 4.3, 'train_steps_per_second': 0.269, 'total_flos': 2109700207742976.0, 'train_loss': 1.078308966292803, 'epoch': 8.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6: The Final Training Run\n",
    "print(f\"\\n{'='*50}\\nSTARTING FINAL RUN: {RUN_NAME}\\n{'='*50}\\n\")\n",
    "\n",
    "# Load tokenizer and tokenize all datasets\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "tokenized_val_ds = val_ds.map(tokenize_fn, batched=True)\n",
    "tokenized_test_ds = test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "# Load the model, authorizing the head replacement\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Setup Differential Learning Rates\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in model.named_parameters() if \"classifier\" not in n], \"lr\": 2e-6},\n",
    "    {\"params\": [p for n, p in model.named_parameters() if \"classifier\" in n], \"lr\": 3e-5},\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters)\n",
    "\n",
    "output_dir = f\"./results/{RUN_NAME}\"\n",
    "training_args = TrainingArguments(output_dir=output_dir, **TRAINING_PARAMS)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    optimizers=(optimizer, None)\n",
    ")\n",
    "\n",
    "# Launch Training!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c8bbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating the final model on the sacred test set ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 23.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "--- EXPERIMENT COMPLETE ---\n",
      "\n",
      "Final Model: final_tune_roberta_augmented_diff_lr\n",
      "Previous Best F1 Score: 0.7352\n",
      "New Best Test F1 Score on Friends Data: 0.6076\n",
      "\n",
      "Your final, best model is saved in: ./results/final_tune_roberta_augmented_diff_lr\n",
      "\n",
      "The model performance was similar to the previous run. This is a solid result, and the model is now more robust due to augmentation!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Final Evaluation\n",
    "print(\"\\n--- Evaluating the final model on the sacred test set ---\")\n",
    "\n",
    "# The trainer automatically loaded the best model from the run because of load_best_model_at_end=True\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_test_ds)\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\\n--- EXPERIMENT COMPLETE ---\\n\")\n",
    "print(f\"Final Model: {RUN_NAME}\")\n",
    "print(f\"Previous Best F1 Score: 0.7352\")\n",
    "print(f\"New Best Test F1 Score on Friends Data: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"\\nYour final, best model is saved in: {output_dir}\")\n",
    "\n",
    "if test_results['eval_f1'] > 0.7352:\n",
    "    print(\"\\nCongratulations! You have successfully improved the model's performance!\")\n",
    "else:\n",
    "    print(\"\\nThe model performance was similar to the previous run. This is a solid result, and the model is now more robust due to augmentation!\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-lab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
