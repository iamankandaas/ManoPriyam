{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1bfac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STEP 1: Preparing Datasets from Excel Files ---\n",
      "Standardized Columns in Friends Data: Index(['text', 'emotion'], dtype='object')\n",
      "Standardized Columns in Kaggle Data: Index(['text', 'emotion'], dtype='object')\n",
      "Train set size: 2509\n",
      "Validation set size: 279\n",
      "Test set (friends only): 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\das99\\AppData\\Local\\Temp\\ipykernel_22744\\92992369.py:95: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 2: Setting up Preprocessing and Class Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2509/2509 [00:01<00:00, 2165.72 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 279/279 [00:00<00:00, 1945.02 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 1605.58 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2509/2509 [00:00<00:00, 384659.29 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 279/279 [00:00<00:00, 110773.46 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 64542.65 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- STEP 3: BASELINE MODEL BAKE-OFF ---\n",
      "\n",
      "==================================================\n",
      "STARTING RUN: baseline_vinai_bertweet-base | MODEL: vinai/bertweet-base\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2509/2509 [00:00<00:00, 4647.80 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 279/279 [00:00<00:00, 4282.08 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 2273.48 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 20%|â–ˆâ–ˆ        | 157/785 [00:47<03:36,  2.90it/s]\n",
      " 20%|â–ˆâ–ˆ        | 157/785 [00:50<03:36,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3656238317489624, 'eval_f1': 0.4993275054905067, 'eval_runtime': 2.3168, 'eval_samples_per_second': 120.424, 'eval_steps_per_second': 15.107, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 314/785 [01:59<02:50,  2.77it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 314/785 [02:03<02:50,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9663352966308594, 'eval_f1': 0.7099553811490019, 'eval_runtime': 3.1937, 'eval_samples_per_second': 87.36, 'eval_steps_per_second': 10.959, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 471/785 [03:07<01:31,  3.41it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 471/785 [03:08<01:31,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.868071973323822, 'eval_f1': 0.7479345638765481, 'eval_runtime': 1.4314, 'eval_samples_per_second': 194.908, 'eval_steps_per_second': 24.451, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 500/785 [03:19<01:13,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2143, 'grad_norm': 8.512378692626953, 'learning_rate': 8.073654390934846e-06, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 628/785 [04:02<00:56,  2.79it/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 628/785 [04:05<00:56,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8703208565711975, 'eval_f1': 0.7456291924396639, 'eval_runtime': 2.5455, 'eval_samples_per_second': 109.605, 'eval_steps_per_second': 13.75, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [05:04<00:00,  3.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [05:07<00:00,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8760215044021606, 'eval_f1': 0.7513380833739648, 'eval_runtime': 2.6922, 'eval_samples_per_second': 103.631, 'eval_steps_per_second': 13.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [05:16<00:00,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 316.3042, 'train_samples_per_second': 39.661, 'train_steps_per_second': 2.482, 'train_loss': 1.0029763580127886, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [05:16<00:00,  2.48it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************** RESULTS FOR RUN: baseline_vinai_bertweet-base ********************\n",
      "Final Test F1 Score on Friends Data: 0.6437\n",
      "\n",
      "==================================================\n",
      "STARTING RUN: baseline_cardiffnlp_twitter-roberta-base-emotion | MODEL: cardiffnlp/twitter-roberta-base-emotion\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2509/2509 [00:00<00:00, 11084.51 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 279/279 [00:00<00:00, 3207.74 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 1905.46 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([4, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 20%|â–ˆâ–ˆ        | 157/785 [01:02<03:51,  2.71it/s]\n",
      " 20%|â–ˆâ–ˆ        | 157/785 [01:04<03:51,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.03691828250885, 'eval_f1': 0.6754469748030677, 'eval_runtime': 2.2547, 'eval_samples_per_second': 123.744, 'eval_steps_per_second': 15.523, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 314/785 [02:08<03:16,  2.40it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 314/785 [02:11<03:16,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8898560404777527, 'eval_f1': 0.7235571183779923, 'eval_runtime': 2.8884, 'eval_samples_per_second': 96.592, 'eval_steps_per_second': 12.117, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 471/785 [03:05<01:40,  3.12it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 471/785 [03:08<01:40,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8357837200164795, 'eval_f1': 0.7482092207851361, 'eval_runtime': 2.2808, 'eval_samples_per_second': 122.325, 'eval_steps_per_second': 15.345, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 500/785 [03:22<01:27,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9776, 'grad_norm': 12.913827896118164, 'learning_rate': 8.073654390934846e-06, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 628/785 [04:06<00:54,  2.90it/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 628/785 [04:08<00:54,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8653876185417175, 'eval_f1': 0.7477334032502476, 'eval_runtime': 2.4658, 'eval_samples_per_second': 113.149, 'eval_steps_per_second': 14.194, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [05:16<00:00,  2.31it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [05:18<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8834185600280762, 'eval_f1': 0.7507998610534573, 'eval_runtime': 2.0465, 'eval_samples_per_second': 136.329, 'eval_steps_per_second': 17.102, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [05:22<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 322.2679, 'train_samples_per_second': 38.927, 'train_steps_per_second': 2.436, 'train_loss': 0.801519133938346, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [05:22<00:00,  2.43it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 20.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************** RESULTS FOR RUN: baseline_cardiffnlp_twitter-roberta-base-emotion ********************\n",
      "Final Test F1 Score on Friends Data: 0.6875\n",
      "\n",
      "\n",
      "--- BAKE-OFF COMPLETE ---\n",
      "Scores: {'vinai/bertweet-base': 0.6436663001065445, 'cardiffnlp/twitter-roberta-base-emotion': 0.6874817441757812}\n",
      "WINNING MODEL: cardiffnlp/twitter-roberta-base-emotion\n",
      "\n",
      "\n",
      "--- STEP 4: HYPERPARAMETER TUNING FOR cardiffnlp/twitter-roberta-base-emotion ---\n",
      "\n",
      "==================================================\n",
      "STARTING RUN: tuning_cardiffnlp_twitter-roberta-base-emotion_run_1 | MODEL: cardiffnlp/twitter-roberta-base-emotion\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2509/2509 [00:00<00:00, 7499.54 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 279/279 [00:00<00:00, 2896.12 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 8348.12 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([4, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 20%|â–ˆâ–ˆ        | 157/785 [00:56<03:38,  2.87it/s]\n",
      " 20%|â–ˆâ–ˆ        | 157/785 [00:59<03:38,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.03691828250885, 'eval_f1': 0.6754469748030677, 'eval_runtime': 2.4777, 'eval_samples_per_second': 112.604, 'eval_steps_per_second': 14.126, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 314/785 [01:56<02:48,  2.80it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 314/785 [01:58<02:48,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8898560404777527, 'eval_f1': 0.7235571183779923, 'eval_runtime': 1.9463, 'eval_samples_per_second': 143.351, 'eval_steps_per_second': 17.983, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 471/785 [02:46<01:11,  4.37it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 471/785 [02:48<01:11,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8357837200164795, 'eval_f1': 0.7482092207851361, 'eval_runtime': 1.4088, 'eval_samples_per_second': 198.044, 'eval_steps_per_second': 24.844, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 500/785 [02:56<01:09,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9776, 'grad_norm': 12.913827896118164, 'learning_rate': 8.073654390934846e-06, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 628/785 [03:42<00:39,  3.96it/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 628/785 [03:44<00:39,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8653876185417175, 'eval_f1': 0.7477334032502476, 'eval_runtime': 2.1947, 'eval_samples_per_second': 127.127, 'eval_steps_per_second': 15.948, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [04:44<00:00,  2.79it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [04:47<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8834185600280762, 'eval_f1': 0.7507998610534573, 'eval_runtime': 3.1645, 'eval_samples_per_second': 88.166, 'eval_steps_per_second': 11.06, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [04:50<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 290.7653, 'train_samples_per_second': 43.145, 'train_steps_per_second': 2.7, 'train_loss': 0.801519133938346, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [04:50<00:00,  2.70it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 23.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************** RESULTS FOR RUN: tuning_cardiffnlp_twitter-roberta-base-emotion_run_1 ********************\n",
      "Final Test F1 Score on Friends Data: 0.6875\n",
      "\n",
      "==================================================\n",
      "STARTING RUN: tuning_cardiffnlp_twitter-roberta-base-emotion_run_2 | MODEL: cardiffnlp/twitter-roberta-base-emotion\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2509/2509 [00:00<00:00, 10518.76 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 279/279 [00:00<00:00, 9506.95 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 5154.23 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([4, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 12%|â–ˆâ–Ž        | 157/1256 [00:53<04:42,  3.88it/s]\n",
      " 12%|â–ˆâ–Ž        | 157/1256 [00:56<04:42,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0137062072753906, 'eval_f1': 0.6784975715127859, 'eval_runtime': 2.2782, 'eval_samples_per_second': 122.467, 'eval_steps_per_second': 15.363, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 314/1256 [02:08<07:58,  1.97it/s]\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 314/1256 [02:10<07:58,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8821491599082947, 'eval_f1': 0.7212448325428287, 'eval_runtime': 1.846, 'eval_samples_per_second': 151.142, 'eval_steps_per_second': 18.96, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 471/1256 [03:03<03:01,  4.32it/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 471/1256 [03:04<03:01,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8608065247535706, 'eval_f1': 0.7489946131852784, 'eval_runtime': 1.4331, 'eval_samples_per_second': 194.686, 'eval_steps_per_second': 24.423, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 501/1256 [03:13<03:04,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9447, 'grad_norm': 19.133054733276367, 'learning_rate': 2.007079646017699e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 628/1256 [03:44<02:28,  4.22it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 628/1256 [03:46<02:28,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9502195715904236, 'eval_f1': 0.7494591907247844, 'eval_runtime': 1.4552, 'eval_samples_per_second': 191.728, 'eval_steps_per_second': 24.052, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 785/1256 [04:26<01:51,  4.23it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 785/1256 [04:27<01:51,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0534489154815674, 'eval_f1': 0.7392917833352347, 'eval_runtime': 1.4375, 'eval_samples_per_second': 194.086, 'eval_steps_per_second': 24.348, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 942/1256 [05:08<01:14,  4.21it/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 942/1256 [05:09<01:14,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1791267395019531, 'eval_f1': 0.7536157774520043, 'eval_runtime': 1.5082, 'eval_samples_per_second': 184.991, 'eval_steps_per_second': 23.207, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1000/1256 [05:25<01:04,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2524, 'grad_norm': 0.9636558890342712, 'learning_rate': 6.79646017699115e-06, 'epoch': 6.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1099/1256 [05:50<00:37,  4.20it/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1099/1256 [05:51<00:37,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.309918999671936, 'eval_f1': 0.7505590424184257, 'eval_runtime': 1.4514, 'eval_samples_per_second': 192.225, 'eval_steps_per_second': 24.114, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1256/1256 [06:32<00:00,  4.19it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1256/1256 [06:33<00:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3308149576187134, 'eval_f1': 0.7599985419960621, 'eval_runtime': 1.4527, 'eval_samples_per_second': 192.055, 'eval_steps_per_second': 24.093, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1256/1256 [06:35<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 395.6131, 'train_samples_per_second': 50.736, 'train_steps_per_second': 3.175, 'train_loss': 0.4977211602933847, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 26.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************** RESULTS FOR RUN: tuning_cardiffnlp_twitter-roberta-base-emotion_run_2 ********************\n",
      "Final Test F1 Score on Friends Data: 0.7137\n",
      "\n",
      "==================================================\n",
      "STARTING RUN: tuning_cardiffnlp_twitter-roberta-base-emotion_run_3 | MODEL: cardiffnlp/twitter-roberta-base-emotion\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2509/2509 [00:00<00:00, 16391.30 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 279/279 [00:00<00:00, 14854.54 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 14196.08 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([4, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 12%|â–ˆâ–Ž        | 157/1256 [00:38<04:20,  4.22it/s]\n",
      " 12%|â–ˆâ–Ž        | 157/1256 [00:39<04:20,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0079212188720703, 'eval_f1': 0.6691762410822509, 'eval_runtime': 1.4462, 'eval_samples_per_second': 192.919, 'eval_steps_per_second': 24.201, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 314/1256 [01:20<03:41,  4.26it/s]\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 314/1256 [01:21<03:41,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8993061184883118, 'eval_f1': 0.7230815343536356, 'eval_runtime': 1.4405, 'eval_samples_per_second': 193.688, 'eval_steps_per_second': 24.298, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 471/1256 [02:01<03:06,  4.22it/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 471/1256 [02:03<03:06,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9878883361816406, 'eval_f1': 0.711335621209246, 'eval_runtime': 1.4527, 'eval_samples_per_second': 192.052, 'eval_steps_per_second': 24.093, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 500/1256 [02:12<03:06,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9131, 'grad_norm': 6.141289234161377, 'learning_rate': 3.345132743362832e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 628/1256 [02:43<02:29,  4.21it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 628/1256 [02:45<02:29,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0376378297805786, 'eval_f1': 0.7267045800273023, 'eval_runtime': 1.4617, 'eval_samples_per_second': 190.88, 'eval_steps_per_second': 23.946, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 785/1256 [03:25<01:52,  4.17it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 785/1256 [03:27<01:52,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1268608570098877, 'eval_f1': 0.7515824076844541, 'eval_runtime': 1.4549, 'eval_samples_per_second': 191.765, 'eval_steps_per_second': 24.057, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 942/1256 [04:07<01:14,  4.20it/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 942/1256 [04:08<01:14,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1340246200561523, 'eval_f1': 0.7816471366327522, 'eval_runtime': 1.455, 'eval_samples_per_second': 191.752, 'eval_steps_per_second': 24.055, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1000/1256 [04:24<01:02,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2114, 'grad_norm': 0.13736696541309357, 'learning_rate': 1.1327433628318584e-05, 'epoch': 6.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1099/1256 [04:49<00:37,  4.22it/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1099/1256 [04:50<00:37,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.25164794921875, 'eval_f1': 0.8033604871938292, 'eval_runtime': 1.4525, 'eval_samples_per_second': 192.083, 'eval_steps_per_second': 24.096, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1256/1256 [05:31<00:00,  4.09it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1256/1256 [05:32<00:00,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3045963048934937, 'eval_f1': 0.7776270078219826, 'eval_runtime': 1.4411, 'eval_samples_per_second': 193.597, 'eval_steps_per_second': 24.286, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1256/1256 [05:35<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 334.8635, 'train_samples_per_second': 59.941, 'train_steps_per_second': 3.751, 'train_loss': 0.4608848763119643, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 25.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************** RESULTS FOR RUN: tuning_cardiffnlp_twitter-roberta-base-emotion_run_3 ********************\n",
      "Final Test F1 Score on Friends Data: 0.7352\n",
      "\n",
      "==================================================\n",
      "STARTING RUN: tuning_cardiffnlp_twitter-roberta-base-emotion_run_4 | MODEL: cardiffnlp/twitter-roberta-base-emotion\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2509/2509 [00:00<00:00, 16850.77 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 279/279 [00:00<00:00, 11358.62 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 13461.40 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([4, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 12%|â–ˆâ–Ž        | 79/632 [00:34<03:31,  2.62it/s]\n",
      " 12%|â–ˆâ–Ž        | 79/632 [00:36<03:31,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.087255835533142, 'eval_f1': 0.633933393655002, 'eval_runtime': 1.4588, 'eval_samples_per_second': 191.247, 'eval_steps_per_second': 23.992, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 158/632 [01:13<03:08,  2.51it/s]\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 158/632 [01:14<03:08,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9204866290092468, 'eval_f1': 0.712683375753676, 'eval_runtime': 1.4757, 'eval_samples_per_second': 189.063, 'eval_steps_per_second': 23.718, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 237/632 [01:51<02:32,  2.58it/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 237/632 [01:52<02:32,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8400378227233887, 'eval_f1': 0.7184390178951897, 'eval_runtime': 1.4821, 'eval_samples_per_second': 188.246, 'eval_steps_per_second': 23.615, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 316/632 [02:28<01:50,  2.86it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 316/632 [02:30<01:50,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8847299814224243, 'eval_f1': 0.7325553785053422, 'eval_runtime': 1.4852, 'eval_samples_per_second': 187.848, 'eval_steps_per_second': 23.565, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 395/632 [03:06<01:28,  2.69it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 395/632 [03:08<01:28,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9525181651115417, 'eval_f1': 0.7524498573739182, 'eval_runtime': 1.4341, 'eval_samples_per_second': 194.548, 'eval_steps_per_second': 24.406, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 474/632 [03:45<01:00,  2.61it/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 474/632 [03:46<01:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9755450487136841, 'eval_f1': 0.7471660749616624, 'eval_runtime': 1.4619, 'eval_samples_per_second': 190.845, 'eval_steps_per_second': 23.941, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 500/632 [03:59<00:56,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7075, 'grad_norm': 1.85690176486969, 'learning_rate': 6.971830985915493e-06, 'epoch': 6.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 553/632 [04:23<00:28,  2.74it/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 553/632 [04:24<00:28,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.039838194847107, 'eval_f1': 0.7460399795347236, 'eval_runtime': 1.4812, 'eval_samples_per_second': 188.354, 'eval_steps_per_second': 23.629, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 632/632 [05:01<00:00,  2.77it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 632/632 [05:02<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0504496097564697, 'eval_f1': 0.7495195699324888, 'eval_runtime': 1.4657, 'eval_samples_per_second': 190.357, 'eval_steps_per_second': 23.88, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 632/632 [05:05<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 305.3529, 'train_samples_per_second': 65.734, 'train_steps_per_second': 2.07, 'train_loss': 0.5965528276902211, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 632/632 [05:05<00:00,  2.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 24.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************** RESULTS FOR RUN: tuning_cardiffnlp_twitter-roberta-base-emotion_run_4 ********************\n",
      "Final Test F1 Score on Friends Data: 0.6380\n",
      "\n",
      "\n",
      "============================================================\n",
      "--- EXPERIMENT COMPLETE ---\n",
      "\n",
      "Best Run: tuning_cardiffnlp_twitter-roberta-base-emotion_run_3\n",
      "Best Hyperparameters: {'learning_rate': 5e-05, 'num_train_epochs': 8, 'per_device_train_batch_size': 16}\n",
      "Best Test F1 Score on Friends Data: 0.7352\n",
      "\n",
      "Your final, best model is saved in: ./results/tuning_cardiffnlp_twitter-roberta-base-emotion_run_3\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run_final_experiment.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "import demoji\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# --- CONFIGURATION SECTION ---\n",
    "\n",
    "# 1. Define the models for the bake-off\n",
    "MODELS_TO_TEST = [\n",
    "    \"vinai/bertweet-base\",\n",
    "    \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "]\n",
    "\n",
    "# 2. Define the hyperparameter search space for the winning model\n",
    "HYPERPARAMETER_SEARCH_SPACE = [\n",
    "    {\"learning_rate\": 2e-5, \"num_train_epochs\": 5, \"per_device_train_batch_size\": 16},\n",
    "    {\"learning_rate\": 3e-5, \"num_train_epochs\": 8, \"per_device_train_batch_size\": 16},\n",
    "    {\"learning_rate\": 5e-5, \"num_train_epochs\": 8, \"per_device_train_batch_size\": 16},\n",
    "    {\"learning_rate\": 3e-5, \"num_train_epochs\": 8, \"per_device_train_batch_size\": 32}, # Try if GPU has enough memory\n",
    "]\n",
    "\n",
    "# 3. Define raw data files\n",
    "FRIENDS_DATA_PATH = '../data/data1.xlsx' # Your 1K high-quality data\n",
    "KAGGLE_DATA_PATHS = ['../data/data2.xlsx', '../data/data3.xlsx'] # The generic data\n",
    "\n",
    "# --- STEP 1: ADVANCED DATA PREPARATION ---\n",
    "\n",
    "# --- STEP 1: ADVANCED DATA PREPARATION ---\n",
    "\n",
    "print(\"--- STEP 1: Preparing Datasets from Excel Files ---\")\n",
    "\n",
    "# Load and combine data\n",
    "df_friends = pd.read_excel(FRIENDS_DATA_PATH)\n",
    "df_kaggle_list = [pd.read_excel(p) for p in KAGGLE_DATA_PATHS]\n",
    "df_kaggle = pd.concat(df_kaggle_list, ignore_index=True)\n",
    "\n",
    "# --- NEW: Standardize and Rename Columns ---\n",
    "# First, make all column names lowercase and remove leading/trailing spaces\n",
    "df_friends.columns = [col.strip().lower() for col in df_friends.columns]\n",
    "df_kaggle.columns = [col.strip().lower() for col in df_kaggle.columns]\n",
    "\n",
    "# Now, rename the 'entry' column to 'text' so the rest of the script works\n",
    "# We will check if the 'entry' column exists before trying to rename it\n",
    "if 'entry' in df_friends.columns:\n",
    "    df_friends.rename(columns={'entry': 'text'}, inplace=True)\n",
    "if 'entry' in df_kaggle.columns:\n",
    "    df_kaggle.rename(columns={'entry': 'text'}, inplace=True)\n",
    "\n",
    "print(\"Standardized Columns in Friends Data:\", df_friends.columns)\n",
    "print(\"Standardized Columns in Kaggle Data:\", df_kaggle.columns)\n",
    "# -------------------------------------------\n",
    "\n",
    "# Clean and drop duplicates (This will now work)\n",
    "df_friends.dropna(subset=['text', 'emotion'], inplace=True)\n",
    "df_kaggle.dropna(subset=['text', 'emotion'], inplace=True)\n",
    "df_friends.drop_duplicates(subset=['text'], inplace=True)\n",
    "df_kaggle.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "# Isolate a \"golden\" test set from your friends' data\n",
    "test_df = df_friends.sample(frac=0.2, random_state=42)\n",
    "\n",
    "# The rest of the data forms the training pool\n",
    "train_pool_df = pd.concat([df_friends.drop(test_df.index), df_kaggle], ignore_index=True)\n",
    "\n",
    "# Split the pool into training and validation sets\n",
    "train_df, val_df = train_test_split(train_pool_df, test_size=0.1, random_state=42, stratify=train_pool_df['emotion'])\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set (friends only): {len(test_df)}\")\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# --- STEP 2: ENHANCED PREPROCESSING & CLASS WEIGHTS ---\n",
    "\n",
    "print(\"\\n--- STEP 2: Setting up Preprocessing and Class Weights ---\")\n",
    "\n",
    "# Initialize demoji for emoji translation\n",
    "demoji.download_codes()\n",
    "\n",
    "def preprocess_text(batch):\n",
    "    # Convert emojis to text descriptions\n",
    "    batch['text'] = [demoji.replace_with_desc(str(text), sep=\" \") for text in batch['text']]\n",
    "    return batch\n",
    "\n",
    "train_ds = train_ds.map(preprocess_text, batched=True)\n",
    "val_ds = val_ds.map(preprocess_text, batched=True)\n",
    "test_ds = test_ds.map(preprocess_text, batched=True)\n",
    "\n",
    "# Create label mappings from the training data\n",
    "unique_labels = train_df['emotion'].unique()\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "def encode_labels(batch):\n",
    "    batch['label'] = [label2id[label] for label in batch['emotion']]\n",
    "    return batch\n",
    "\n",
    "train_ds = train_ds.map(encode_labels, batched=True)\n",
    "val_ds = val_ds.map(encode_labels, batched=True)\n",
    "test_ds = test_ds.map(encode_labels, batched=True)\n",
    "\n",
    "# Calculate class weights for handling imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(list(label2id.keys())),\n",
    "    y=train_df['emotion']\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\")\n",
    "\n",
    "# Create a Custom Trainer to use the class weights\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Define a shared metric computation function\n",
    "metric = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "\n",
    "# --- CORE TRAINING FUNCTION ---\n",
    "\n",
    "def train_and_evaluate(model_checkpoint, training_args_dict, run_name):\n",
    "    print(f\"\\n{'='*50}\\nSTARTING RUN: {run_name} | MODEL: {model_checkpoint}\\n{'='*50}\\n\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "    tokenized_train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "    tokenized_val_ds = val_ds.map(tokenize_fn, batched=True)\n",
    "    tokenized_test_ds = test_ds.map(tokenize_fn, batched=True)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True  # <-- ADD THIS LINE\n",
    "    )\n",
    "    \n",
    "    output_dir = f\"./results/{run_name}\"\n",
    "    training_args = TrainingArguments(output_dir=output_dir, **training_args_dict)\n",
    "\n",
    "    trainer = CustomTrainer( # Use the CustomTrainer with class weights\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_ds,\n",
    "        eval_dataset=tokenized_val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate on the sacred, friend-only test set\n",
    "    test_results = trainer.evaluate(eval_dataset=tokenized_test_ds)\n",
    "    print(f\"\\n{'*'*20} RESULTS FOR RUN: {run_name} {'*'*20}\")\n",
    "    print(f\"Final Test F1 Score on Friends Data: {test_results['eval_f1']:.4f}\")\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# --- MAIN EXECUTION SCRIPT ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- STEP 3: The Model Bake-Off ---\n",
    "    print(\"\\n\\n--- STEP 3: BASELINE MODEL BAKE-OFF ---\")\n",
    "    \n",
    "    baseline_results = {}\n",
    "    baseline_args = {\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"num_train_epochs\": 5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"f1\",\n",
    "        \"save_total_limit\": 1, # Only keep the single best checkpoint\n",
    "        \"lr_scheduler_type\": 'linear',\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "\n",
    "    for model_name in MODELS_TO_TEST:\n",
    "        run_name = f\"baseline_{model_name.replace('/', '_')}\"\n",
    "        results = train_and_evaluate(model_name, baseline_args, run_name)\n",
    "        baseline_results[model_name] = results['eval_f1']\n",
    "\n",
    "    winning_model = max(baseline_results, key=baseline_results.get)\n",
    "    print(f\"\\n\\n--- BAKE-OFF COMPLETE ---\\nScores: {baseline_results}\\nWINNING MODEL: {winning_model}\\n\\n\")\n",
    "\n",
    "    # --- STEP 4: Hyperparameter Tuning the Winner ---\n",
    "    print(f\"--- STEP 4: HYPERPARAMETER TUNING FOR {winning_model} ---\")\n",
    "    \n",
    "    tuning_results = []\n",
    "    \n",
    "    for i, params in enumerate(HYPERPARAMETER_SEARCH_SPACE):\n",
    "        run_args = baseline_args.copy()\n",
    "        run_args.update(params)\n",
    "        \n",
    "        run_name = f\"tuning_{winning_model.replace('/', '_')}_run_{i+1}\"\n",
    "        results = train_and_evaluate(winning_model, run_args, run_name)\n",
    "        \n",
    "        tuning_results.append({\n",
    "            \"run_name\": run_name,\n",
    "            \"params\": params,\n",
    "            \"f1_score\": results['eval_f1']\n",
    "        })\n",
    "\n",
    "    best_run = max(tuning_results, key=lambda x: x['f1_score'])\n",
    "    \n",
    "    print(f\"\\n\\n{'='*60}\\n--- EXPERIMENT COMPLETE ---\\n\")\n",
    "    print(f\"Best Run: {best_run['run_name']}\")\n",
    "    print(f\"Best Hyperparameters: {best_run['params']}\")\n",
    "    print(f\"Best Test F1 Score on Friends Data: {best_run['f1_score']:.4f}\")\n",
    "    print(f\"\\nYour final, best model is saved in: ./results/{best_run['run_name']}\")\n",
    "    print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-lab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
