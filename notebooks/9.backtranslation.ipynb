{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "488bfb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete. Setup ready for the final experiment.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    pipeline # Import the pipeline for back-translation\n",
    ")\n",
    "\n",
    "print(\"Imports complete. Setup ready for the final experiment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f80086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading translation models for back-translation (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation models loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration and Back-Translation Augmentation\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Using the same powerful model as last time\n",
    "MODEL_CHECKPOINT = \"cardiffnlp/twitter-roberta-base-emotion-multilabel-latest\"\n",
    "RUN_NAME = \"final_run_clean_data_backtranslation\"\n",
    "\n",
    "# --- Define paths to your data ---\n",
    "FRIENDS_DATA_PATH = '../data/data1.xlsx' # <-- Using your newly cleaned file!\n",
    "KAGGLE_DATA_PATHS = ['../data/data2.xlsx', '../data/data3.xlsx']\n",
    "\n",
    "# --- Setup Back-Translation Augmenter using transformers ---\n",
    "print(\"Loading translation models for back-translation (this may take a moment)...\")\n",
    "translator_to_de = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-de\", device=0 if torch.cuda.is_available() else -1)\n",
    "translator_to_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-de-en\", device=0 if torch.cuda.is_available() else -1)\n",
    "print(\"Translation models loaded.\")\n",
    "\n",
    "def augment_with_backtranslation(df, num_augmented_samples=1000):\n",
    "    augmented_texts = []\n",
    "    original_df = df.copy().dropna(subset=['text'])\n",
    "\n",
    "    print(f\"Generating {num_augmented_samples} new samples via Back-Translation...\")\n",
    "    if len(original_df) == 0:\n",
    "        print(\"Warning: DataFrame is empty. Skipping augmentation.\")\n",
    "        return original_df\n",
    "\n",
    "    for i in range(num_augmented_samples):\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  ...augmented {i+1}/{num_augmented_samples}\")\n",
    "            \n",
    "        sample = original_df.sample(1)\n",
    "        original_text = sample['text'].iloc[0]\n",
    "        original_emotion = sample['emotion'].iloc[0]\n",
    "\n",
    "        # Translate to German and back to English to create a paraphrase\n",
    "        translated = translator_to_de(original_text, max_length=128)[0]['translation_text']\n",
    "        paraphrase = translator_to_en(translated, max_length=128)[0]['translation_text']\n",
    "        \n",
    "        augmented_texts.append({'text': paraphrase, 'emotion': original_emotion})\n",
    "        \n",
    "    return pd.concat([original_df, pd.DataFrame(augmented_texts)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62923bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 1: Preparing Datasets with Clean Data and Augmentation ---\n",
      "Generating 1000 new samples via Back-Translation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Your input_length: 174 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 127 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ...augmented 50/1000\n",
      "  ...augmented 100/1000\n",
      "  ...augmented 150/1000\n",
      "  ...augmented 200/1000\n",
      "  ...augmented 250/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 178 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 127 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ...augmented 300/1000\n",
      "  ...augmented 350/1000\n",
      "  ...augmented 400/1000\n",
      "  ...augmented 450/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 186 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 127 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ...augmented 500/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 120 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ...augmented 550/1000\n",
      "  ...augmented 600/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 174 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 127 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ...augmented 650/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 182 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 127 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 120 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ...augmented 700/1000\n",
      "  ...augmented 750/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 184 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 127 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ...augmented 800/1000\n",
      "  ...augmented 850/1000\n",
      "  ...augmented 900/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 192 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 127 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 198 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 127 is bigger than 0.9 * max_length: 128. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ...augmented 950/1000\n",
      "  ...augmented 1000/1000\n",
      "\n",
      "Augmented General Training Pool size: 3509\n",
      "Specialist Training set (friends only): 801\n",
      "Validation set size: 279\n",
      "Sacred Test set (friends only): 200\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Full Data Preparation with Back-Translation\n",
    "\n",
    "print(\"\\n--- STEP 1: Preparing Datasets with Clean Data and Augmentation ---\")\n",
    "\n",
    "# Load and combine data\n",
    "df_friends = pd.read_excel(FRIENDS_DATA_PATH)\n",
    "df_kaggle_list = [pd.read_excel(p) for p in KAGGLE_DATA_PATHS]\n",
    "df_kaggle = pd.concat(df_kaggle_list, ignore_index=True)\n",
    "\n",
    "# Standardize and rename columns\n",
    "df_friends.columns = [col.strip().lower() for col in df_friends.columns]\n",
    "df_kaggle.columns = [col.strip().lower() for col in df_kaggle.columns]\n",
    "if 'entry' in df_friends.columns: df_friends.rename(columns={'entry': 'text'}, inplace=True)\n",
    "if 'entry' in df_kaggle.columns: df_kaggle.rename(columns={'entry': 'text'}, inplace=True)\n",
    "\n",
    "# Clean and drop duplicates\n",
    "df_friends.dropna(subset=['text', 'emotion'], inplace=True)\n",
    "df_kaggle.dropna(subset=['text', 'emotion'], inplace=True)\n",
    "df_friends.drop_duplicates(subset=['text'], inplace=True)\n",
    "df_kaggle.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "# Create the same strategic split as before\n",
    "test_df = df_friends.sample(frac=0.2, random_state=42)\n",
    "train_friends_df = df_friends.drop(test_df.index) # Specialist data\n",
    "train_pool_df = pd.concat([train_friends_df, df_kaggle], ignore_index=True)\n",
    "train_pool_df, val_df = train_test_split(train_pool_df, test_size=0.1, random_state=42, stratify=train_pool_df['emotion'])\n",
    "\n",
    "# --- APPLY BACK-TRANSLATION AUGMENTATION ---\n",
    "train_pool_df = augment_with_backtranslation(train_pool_df)\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_pool_ds = Dataset.from_pandas(train_pool_df)\n",
    "train_friends_ds = Dataset.from_pandas(train_friends_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(f\"\\nAugmented General Training Pool size: {len(train_pool_df)}\")\n",
    "print(f\"Specialist Training set (friends only): {len(train_friends_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Sacred Test set (friends only): {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c7a3eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 2: Final Preprocessing and Setup ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3509/3509 [00:00<00:00, 447281.96 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 801/801 [00:00<00:00, 160181.06 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 279/279 [00:00<00:00, 92984.57 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 80427.69 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3509/3509 [00:00<00:00, 23212.28 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 801/801 [00:00<00:00, 14689.19 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 279/279 [00:00<00:00, 17422.15 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 15429.03 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Ready for the ultimate Two-Stage Training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Preprocessing, Custom Trainer, and Tokenization\n",
    "\n",
    "print(\"\\n--- STEP 2: Final Preprocessing and Setup ---\")\n",
    "\n",
    "# Create label mappings from the full training data\n",
    "unique_labels = sorted(train_pool_df['emotion'].unique())\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "# --- CRITICAL CHANGE: We no longer translate emojis ---\n",
    "def preprocess_and_encode(batch):\n",
    "    # The text remains as is, with raw emojis.\n",
    "    # We only encode the labels.\n",
    "    batch['label'] = [label2id[label] for label in batch['emotion']]\n",
    "    return batch\n",
    "\n",
    "# Apply preprocessing to all datasets\n",
    "train_pool_ds = train_pool_ds.map(preprocess_and_encode, batched=True, remove_columns=['emotion'])\n",
    "train_friends_ds = train_friends_ds.map(preprocess_and_encode, batched=True, remove_columns=['emotion'])\n",
    "val_ds = val_ds.map(preprocess_and_encode, batched=True, remove_columns=['emotion'])\n",
    "test_ds = test_ds.map(preprocess_and_encode, batched=True, remove_columns=['emotion'])\n",
    "\n",
    "# Calculate class weights for handling imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(unique_labels),\n",
    "    y=train_pool_df['emotion']\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\")\n",
    "\n",
    "# Create a Custom Trainer to use the class weights\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Define a shared metric computation function\n",
    "metric = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "\n",
    "# Initialize tokenizer and tokenize datasets\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_train_pool_ds = train_pool_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "tokenized_train_friends_ds = train_friends_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "tokenized_val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "tokenized_test_ds = test_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "\n",
    "print(\"Setup complete. Ready for the ultimate Two-Stage Training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00bd8d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STARTING TWO-STAGE TRAINING: final_run_clean_data_backtranslation\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion-multilabel-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 1: Training on General Pool (Cleaned + Kaggle + Back-Translation) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|â–ˆâ–ˆ        | 220/1100 [00:51<02:48,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9580445885658264, 'eval_f1': 0.668790288439472, 'eval_runtime': 1.2761, 'eval_samples_per_second': 218.638, 'eval_steps_per_second': 27.428, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 440/1100 [01:59<03:48,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9182518124580383, 'eval_f1': 0.6670591323691726, 'eval_runtime': 2.2989, 'eval_samples_per_second': 121.365, 'eval_steps_per_second': 15.225, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 501/1100 [02:19<02:14,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9189, 'grad_norm': 16.875532150268555, 'learning_rate': 1.6363636363636363e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 660/1100 [02:56<01:25,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9603327512741089, 'eval_f1': 0.7056607869709697, 'eval_runtime': 1.3256, 'eval_samples_per_second': 210.472, 'eval_steps_per_second': 26.403, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 880/1100 [03:52<00:44,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0790836811065674, 'eval_f1': 0.7138750894114849, 'eval_runtime': 1.3765, 'eval_samples_per_second': 202.695, 'eval_steps_per_second': 25.428, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1000/1100 [04:23<00:24,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3522, 'grad_norm': 0.9848214387893677, 'learning_rate': 2.7272727272727272e-06, 'epoch': 4.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [04:48<00:00,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1269820928573608, 'eval_f1': 0.7216312957439908, 'eval_runtime': 1.3761, 'eval_samples_per_second': 202.749, 'eval_steps_per_second': 25.434, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [04:50<00:00,  3.78it/s]\n",
      "d:\\IITG\\other courses\\Manopriyam\\nlp-lab-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 290.7028, 'train_samples_per_second': 60.354, 'train_steps_per_second': 3.784, 'train_loss': 0.5980449728532271, 'epoch': 5.0}\n",
      "\n",
      "--- STAGE 2: Fine-tuning on Specialist Data (Cleaned Friends Only) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–Ž        | 101/808 [00:23<02:34,  4.56it/s]\n",
      " 12%|â–ˆâ–Ž        | 101/808 [00:24<02:34,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9879629611968994, 'eval_f1': 0.7654741971361175, 'eval_runtime': 1.3213, 'eval_samples_per_second': 211.16, 'eval_steps_per_second': 26.49, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|â–ˆâ–ˆâ–Œ       | 202/808 [00:50<02:09,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0310834646224976, 'eval_f1': 0.7844232648781769, 'eval_runtime': 1.3356, 'eval_samples_per_second': 208.9, 'eval_steps_per_second': 26.206, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 303/808 [01:15<01:49,  4.60it/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 303/808 [01:17<01:49,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0360987186431885, 'eval_f1': 0.8111528382969713, 'eval_runtime': 1.3089, 'eval_samples_per_second': 213.157, 'eval_steps_per_second': 26.74, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 404/808 [01:42<01:30,  4.45it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 404/808 [01:43<01:30,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1280171871185303, 'eval_f1': 0.8186603584788058, 'eval_runtime': 1.3048, 'eval_samples_per_second': 213.833, 'eval_steps_per_second': 26.825, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 501/808 [02:07<01:10,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1769, 'grad_norm': 0.14687153697013855, 'learning_rate': 3.8118811881188123e-06, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 505/808 [02:08<01:04,  4.70it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 505/808 [02:10<01:04,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1726030111312866, 'eval_f1': 0.8075795906522949, 'eval_runtime': 1.3211, 'eval_samples_per_second': 211.188, 'eval_steps_per_second': 26.493, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 606/808 [02:35<00:41,  4.87it/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 606/808 [02:36<00:41,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2375930547714233, 'eval_f1': 0.7939227090350217, 'eval_runtime': 1.3136, 'eval_samples_per_second': 212.4, 'eval_steps_per_second': 26.645, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 707/808 [03:01<00:22,  4.51it/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 707/808 [03:03<00:22,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2343744039535522, 'eval_f1': 0.8071103999881813, 'eval_runtime': 1.2942, 'eval_samples_per_second': 215.574, 'eval_steps_per_second': 27.043, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 707/808 [03:05<00:26,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 185.3493, 'train_samples_per_second': 34.573, 'train_steps_per_second': 4.359, 'train_loss': 0.137194749489532, 'epoch': 7.0}\n",
      "\n",
      "--- Evaluating the final SPECIALIST model on the sacred test set ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 27.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "--- FINAL EXPERIMENT COMPLETE ---\n",
      "\n",
      "Final Model: final_run_clean_data_backtranslation\n",
      "Previous Best F1 Score: 0.7250\n",
      "Final Specialist F1 Score on Friends Data: 0.7570\n",
      "\n",
      "Your final, best specialist model is saved in: ./results/final_run_clean_data_backtranslation\n",
      "\n",
      "CHAMPION! This is the new best model!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Two-Stage Training and Final Evaluation\n",
    "\n",
    "print(f\"\\n{'='*50}\\nSTARTING TWO-STAGE TRAINING: {RUN_NAME}\\n{'='*50}\\n\")\n",
    "\n",
    "# Load the initial model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "output_dir = f\"./results/{RUN_NAME}\"\n",
    "\n",
    "# --- STAGE 1: Generalist Training on Augmented Data ---\n",
    "print(\"\\n--- STAGE 1: Training on General Pool (Cleaned + Kaggle + Back-Translation) ---\")\n",
    "stage1_args = TrainingArguments(\n",
    "    output_dir=f\"{output_dir}/stage1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer_stage1 = CustomTrainer(\n",
    "    model=model,\n",
    "    args=stage1_args,\n",
    "    train_dataset=tokenized_train_pool_ds,\n",
    "    eval_dataset=tokenized_val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "trainer_stage1.train()\n",
    "\n",
    "\n",
    "# --- STAGE 2: Specialist Training on Cleaned Friends Data ---\n",
    "print(\"\\n--- STAGE 2: Fine-tuning on Specialist Data (Cleaned Friends Only) ---\")\n",
    "stage2_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer_stage2 = CustomTrainer(\n",
    "    model=trainer_stage1.model,\n",
    "    args=stage2_args,\n",
    "    train_dataset=tokenized_train_friends_ds,\n",
    "    eval_dataset=tokenized_val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "trainer_stage2.train()\n",
    "\n",
    "# --- FINAL EVALUATION ---\n",
    "print(\"\\n--- Evaluating the final SPECIALIST model on the sacred test set ---\")\n",
    "test_results = trainer_stage2.evaluate(eval_dataset=tokenized_test_ds)\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\\n--- FINAL EXPERIMENT COMPLETE ---\\n\")\n",
    "print(f\"Final Model: {RUN_NAME}\")\n",
    "print(f\"Previous Best F1 Score: 0.7250\")\n",
    "print(f\"Final Specialist F1 Score on Friends Data: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"\\nYour final, best specialist model is saved in: {output_dir}\")\n",
    "\n",
    "if test_results['eval_f1'] > 0.7250:\n",
    "    print(\"\\nCHAMPION! This is the new best model!\")\n",
    "else:\n",
    "    print(\"\\nPerformance was similar. The cleaned data and new augmentation have made the model more robust.\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-lab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
